{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investors' Sentiment & S&P500 : Model & Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Organisation\n",
    "1. Data Collection - Subreddit\n",
    "2. Data Collection - Target \n",
    "3. Merging Data\n",
    "4. EDA and Preprocessing \n",
    "5. **Model Tuning and Insights (Current Notebook)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, plot_roc_curve,\\\n",
    "                            roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('ticks')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Title on Daily Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailytitle_df = pd.read_csv(\"data/project_df_dailytitle_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>combined_title</th>\n",
       "      <th>percent_change_class</th>\n",
       "      <th>title_len</th>\n",
       "      <th>title_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Backtesting moving average crossover Can someo...</td>\n",
       "      <td>up</td>\n",
       "      <td>108</td>\n",
       "      <td>backtesting moving average crossover someone e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>What’s your best performing stock today? How t...</td>\n",
       "      <td>up</td>\n",
       "      <td>503</td>\n",
       "      <td>best performing stock today explain option new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>Come Join Quantum Stock Trading! Help reach fi...</td>\n",
       "      <td>up</td>\n",
       "      <td>209</td>\n",
       "      <td>come join quantum stock trading help reach fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>Greenspan says stock market \"is still a bit to...</td>\n",
       "      <td>up</td>\n",
       "      <td>541</td>\n",
       "      <td>greenspan say stock market still bit high appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>ONC, ONCY &amp;amp; ONC.WT What is everyone's opin...</td>\n",
       "      <td>up</td>\n",
       "      <td>472</td>\n",
       "      <td>onc oncy amp onc wt everyone opinion company d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                     combined_title  \\\n",
       "0  2019-01-03  Backtesting moving average crossover Can someo...   \n",
       "1  2019-01-04  What’s your best performing stock today? How t...   \n",
       "2  2019-01-07  Come Join Quantum Stock Trading! Help reach fi...   \n",
       "3  2019-01-08  Greenspan says stock market \"is still a bit to...   \n",
       "4  2019-01-09  ONC, ONCY &amp; ONC.WT What is everyone's opin...   \n",
       "\n",
       "  percent_change_class  title_len  \\\n",
       "0                   up        108   \n",
       "1                   up        503   \n",
       "2                   up        209   \n",
       "3                   up        541   \n",
       "4                   up        472   \n",
       "\n",
       "                                       title_cleaned  \n",
       "0  backtesting moving average crossover someone e...  \n",
       "1  best performing stock today explain option new...  \n",
       "2  come join quantum stock trading help reach fin...  \n",
       "3  greenspan say stock market still bit high appl...  \n",
       "4  onc oncy amp onc wt everyone opinion company d...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailytitle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailytitle_df['percent_change_class'] = dailytitle_df['percent_change_class'].map({'up' : 1, 'down': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 502 entries, 0 to 501\n",
      "Data columns (total 5 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Date                  502 non-null    object\n",
      " 1   combined_title        502 non-null    object\n",
      " 2   percent_change_class  502 non-null    int64 \n",
      " 3   title_len             502 non-null    int64 \n",
      " 4   title_cleaned         502 non-null    object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 19.7+ KB\n"
     ]
    }
   ],
   "source": [
    "dailytitle_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                    0\n",
       "combined_title          0\n",
       "percent_change_class    0\n",
       "title_len               0\n",
       "title_cleaned           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailytitle_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailytitle_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Title & Post in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlepost_df = pd.read_csv(\"data/project_titlepost2019_df_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>is_self</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>permalink</th>\n",
       "      <th>author</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>Date</th>\n",
       "      <th>year_and_month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>percent_change_class</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>first_title_cleaned</th>\n",
       "      <th>first_selftext_cleaned</th>\n",
       "      <th>final_title_cleaned</th>\n",
       "      <th>final_selftext_cleaned</th>\n",
       "      <th>fresh_meat</th>\n",
       "      <th>lean_meat</th>\n",
       "      <th>lean_meat_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-03 18:02:40</td>\n",
       "      <td>Backtesting moving average crossover</td>\n",
       "      <td>Hello guys,  \\nI was trying to backtest a movi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/r/stocks/comments/ac4c0u/backtesting_moving_a...</td>\n",
       "      <td>bhandarimohit2029</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01-03 00:00:00</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>1</td>\n",
       "      <td>up</td>\n",
       "      <td>4</td>\n",
       "      <td>212</td>\n",
       "      <td>backtesting moving average crossover</td>\n",
       "      <td>hello guy trying backtest moving average cross...</td>\n",
       "      <td>backtesting moving crossover</td>\n",
       "      <td>hello backtest moving crossover indian found i...</td>\n",
       "      <td>backtesting moving average crossover hello guy...</td>\n",
       "      <td>backtesting moving crossover hello backtest mo...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-03 20:12:03</td>\n",
       "      <td>Can someone ELI5 what a cross signal Index is ...</td>\n",
       "      <td>I just finished Margin call on Netflix and was...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/r/stocks/comments/ac55o3/can_someone_eli5_wha...</td>\n",
       "      <td>tellmetheworld</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01-03 00:00:00</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>1</td>\n",
       "      <td>up</td>\n",
       "      <td>16</td>\n",
       "      <td>59</td>\n",
       "      <td>someone eli cross signal index used trading</td>\n",
       "      <td>finished margin call netflix intrigued using i...</td>\n",
       "      <td>someone eli cross used</td>\n",
       "      <td>call netflix intrigued indicator sort proof sk...</td>\n",
       "      <td>someone eli cross signal index used trading fi...</td>\n",
       "      <td>someone eli cross used call netflix intrigued ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-03 20:54:22</td>\n",
       "      <td>Your AM Global Stocks Preview and a whole lot ...</td>\n",
       "      <td>\\n\\n### US Stocks\\n\\n* **US stocks index futu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>/r/stocks/comments/ac5gf6/your_am_global_stock...</td>\n",
       "      <td>QuantalyticsResearch</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01-03 00:00:00</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>1</td>\n",
       "      <td>up</td>\n",
       "      <td>24</td>\n",
       "      <td>1033</td>\n",
       "      <td>global stock preview whole lot news need read ...</td>\n",
       "      <td>u stock u stock index future dropping sharply ...</td>\n",
       "      <td>preview whole negative guidance aapl spook</td>\n",
       "      <td>u u dropping sharply front p negative guidance...</td>\n",
       "      <td>global stock preview whole lot news need read ...</td>\n",
       "      <td>preview whole negative guidance aapl spook u u...</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-03 21:01:23</td>\n",
       "      <td>Gld outperforms snp and Dow</td>\n",
       "      <td>Gld has outperformed the snp and Dow since 200...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>/r/stocks/comments/ac5ibv/gld_outperforms_snp_...</td>\n",
       "      <td>1anon2y3mous</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01-03 00:00:00</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>1</td>\n",
       "      <td>up</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>gld outperforms snp dow</td>\n",
       "      <td>gld outperformed snp dow since gold bad rap in...</td>\n",
       "      <td>gld outperforms snp</td>\n",
       "      <td>gld outperformed snp since rap</td>\n",
       "      <td>gld outperforms snp dow gld outperformed snp d...</td>\n",
       "      <td>gld outperforms snp gld outperformed snp since...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-03 21:17:40</td>\n",
       "      <td>Alternative to Yahoo Finance</td>\n",
       "      <td>Hi there, i switched from Android to iOS.\\n\\nI...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>/r/stocks/comments/ac5mqv/alternative_to_yahoo...</td>\n",
       "      <td>h0ly88</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01-03 00:00:00</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>1</td>\n",
       "      <td>up</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>alternative yahoo finance</td>\n",
       "      <td>hi switched android io always used mystocks ap...</td>\n",
       "      <td>alternative</td>\n",
       "      <td>switched android always used mystocks android ...</td>\n",
       "      <td>alternative yahoo finance hi switched android ...</td>\n",
       "      <td>alternative switched android always used mysto...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              title  \\\n",
       "0  2019-01-03 18:02:40               Backtesting moving average crossover   \n",
       "1  2019-01-03 20:12:03  Can someone ELI5 what a cross signal Index is ...   \n",
       "2  2019-01-03 20:54:22  Your AM Global Stocks Preview and a whole lot ...   \n",
       "3  2019-01-03 21:01:23                        Gld outperforms snp and Dow   \n",
       "4  2019-01-03 21:17:40                       Alternative to Yahoo Finance   \n",
       "\n",
       "                                            selftext  is_self  upvotes  \\\n",
       "0  Hello guys,  \\nI was trying to backtest a movi...      1.0      1.0   \n",
       "1  I just finished Margin call on Netflix and was...      1.0      1.0   \n",
       "2   \\n\\n### US Stocks\\n\\n* **US stocks index futu...      1.0      1.0   \n",
       "3  Gld has outperformed the snp and Dow since 200...      1.0      1.0   \n",
       "4  Hi there, i switched from Android to iOS.\\n\\nI...      1.0      1.0   \n",
       "\n",
       "   n_comments                                          permalink  \\\n",
       "0         0.0  /r/stocks/comments/ac4c0u/backtesting_moving_a...   \n",
       "1         0.0  /r/stocks/comments/ac55o3/can_someone_eli5_wha...   \n",
       "2        16.0  /r/stocks/comments/ac5gf6/your_am_global_stock...   \n",
       "3        30.0  /r/stocks/comments/ac5ibv/gld_outperforms_snp_...   \n",
       "4        10.0  /r/stocks/comments/ac5mqv/alternative_to_yahoo...   \n",
       "\n",
       "                 author  month  year                 Date year_and_month  \\\n",
       "0     bhandarimohit2029      1  2019  2019-01-03 00:00:00        2019-01   \n",
       "1        tellmetheworld      1  2019  2019-01-03 00:00:00        2019-01   \n",
       "2  QuantalyticsResearch      1  2019  2019-01-03 00:00:00        2019-01   \n",
       "3          1anon2y3mous      1  2019  2019-01-03 00:00:00        2019-01   \n",
       "4                h0ly88      1  2019  2019-01-03 00:00:00        2019-01   \n",
       "\n",
       "   week_of_year percent_change_class  title_len  text_len  \\\n",
       "0             1                   up          4       212   \n",
       "1             1                   up         16        59   \n",
       "2             1                   up         24      1033   \n",
       "3             1                   up          5        20   \n",
       "4             1                   up          4        65   \n",
       "\n",
       "                                 first_title_cleaned  \\\n",
       "0               backtesting moving average crossover   \n",
       "1        someone eli cross signal index used trading   \n",
       "2  global stock preview whole lot news need read ...   \n",
       "3                            gld outperforms snp dow   \n",
       "4                          alternative yahoo finance   \n",
       "\n",
       "                              first_selftext_cleaned  \\\n",
       "0  hello guy trying backtest moving average cross...   \n",
       "1  finished margin call netflix intrigued using i...   \n",
       "2  u stock u stock index future dropping sharply ...   \n",
       "3  gld outperformed snp dow since gold bad rap in...   \n",
       "4  hi switched android io always used mystocks ap...   \n",
       "\n",
       "                          final_title_cleaned  \\\n",
       "0                backtesting moving crossover   \n",
       "1                      someone eli cross used   \n",
       "2  preview whole negative guidance aapl spook   \n",
       "3                         gld outperforms snp   \n",
       "4                                 alternative   \n",
       "\n",
       "                              final_selftext_cleaned  \\\n",
       "0  hello backtest moving crossover indian found i...   \n",
       "1  call netflix intrigued indicator sort proof sk...   \n",
       "2  u u dropping sharply front p negative guidance...   \n",
       "3                     gld outperformed snp since rap   \n",
       "4  switched android always used mystocks android ...   \n",
       "\n",
       "                                          fresh_meat  \\\n",
       "0  backtesting moving average crossover hello guy...   \n",
       "1  someone eli cross signal index used trading fi...   \n",
       "2  global stock preview whole lot news need read ...   \n",
       "3  gld outperforms snp dow gld outperformed snp d...   \n",
       "4  alternative yahoo finance hi switched android ...   \n",
       "\n",
       "                                           lean_meat  lean_meat_len  \n",
       "0  backtesting moving crossover hello backtest mo...             85  \n",
       "1  someone eli cross used call netflix intrigued ...             24  \n",
       "2  preview whole negative guidance aapl spook u u...            327  \n",
       "3  gld outperforms snp gld outperformed snp since...              8  \n",
       "4  alternative switched android always used mysto...             23  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlepost_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlepost_df['percent_change_class'] = titlepost_df['percent_change_class'].map({'up' : 1, 'down': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67482 entries, 0 to 67481\n",
      "Data columns (total 23 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   date                    67482 non-null  object \n",
      " 1   title                   67442 non-null  object \n",
      " 2   selftext                66717 non-null  object \n",
      " 3   is_self                 67482 non-null  float64\n",
      " 4   upvotes                 67482 non-null  float64\n",
      " 5   n_comments              67482 non-null  float64\n",
      " 6   permalink               67482 non-null  object \n",
      " 7   author                  67482 non-null  object \n",
      " 8   month                   67482 non-null  int64  \n",
      " 9   year                    67482 non-null  int64  \n",
      " 10  Date                    67482 non-null  object \n",
      " 11  year_and_month          67482 non-null  object \n",
      " 12  week_of_year            67482 non-null  int64  \n",
      " 13  percent_change_class    67482 non-null  int64  \n",
      " 14  title_len               67482 non-null  int64  \n",
      " 15  text_len                67482 non-null  int64  \n",
      " 16  first_title_cleaned     67214 non-null  object \n",
      " 17  first_selftext_cleaned  66231 non-null  object \n",
      " 18  final_title_cleaned     64085 non-null  object \n",
      " 19  final_selftext_cleaned  37733 non-null  object \n",
      " 20  fresh_meat              67482 non-null  object \n",
      " 21  lean_meat               67482 non-null  object \n",
      " 22  lean_meat_len           67482 non-null  int64  \n",
      "dtypes: float64(3), int64(7), object(13)\n",
      "memory usage: 11.8+ MB\n"
     ]
    }
   ],
   "source": [
    "titlepost_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67482, 23)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlepost_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-43424ef0-427a-4caf-b39e-6be72b73f215",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Function for Modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instiantiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cell_id": "00013-f2ab5c24-b4e0-4d59-9a36-84d86ce1486b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1610262368205,
    "output_cleared": false,
    "source_hash": "4c63c663",
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {'lr': LogisticRegression(random_state=42),\n",
    "          'nb': MultinomialNB(),\n",
    "          'rf': RandomForestClassifier(random_state=42),\n",
    "          'ada': AdaBoostClassifier(random_state=42)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = {'cvec': CountVectorizer(),\n",
    "               'tvec': TfidfVectorizer()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizers Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_params = {\n",
    "    # Setting a limit of n-number of features included/vocab size\n",
    "    'cvec__max_features': [None, 100],\n",
    "\n",
    "    # Setting a minimum number of times the word/token has to appear in n-documents\n",
    "    'cvec__min_df':[2, 4],\n",
    "    # Setting an upper threshold/max percentage of n% of documents from corpus \n",
    "    'cvec__max_df': [0.2, 0.95],\n",
    "    \n",
    "    # With stopwords\n",
    "    'cvec__stop_words': ['english'],\n",
    "    \n",
    "    # Testing with bigrams and trigrams\n",
    "    'cvec__ngram_range':[(1,1) , (1,2), (2,2), (2,3), (3,3)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_params = {\n",
    "    'tvec__max_features': [None, 100],\n",
    "    'tvec__min_df':[3, 5],\n",
    "    'tvec__max_df': [0.2, 0.4],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__ngram_range':[(1,1) , (1,2), (2,2), (2,3), (3,3)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    # Trying different types of regularization\n",
    "    'lr__penalty':['l2','l1'],\n",
    "\n",
    "     # Trying different alphas of: 10, 1, 0.1 (C = 1/alpha)\n",
    "    'lr__C':[0.1, 1, 10],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_params = {\n",
    "    'nb__fit_prior': [True, False],\n",
    "    'nb__alpha': [0, 0.4, 0.8]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model Tuning Hyperparameters\n",
    "# source: https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/\n",
    "# source: https://builtin.com/data-science/random-forest-algorithm\n",
    "\n",
    "rf_params = {\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [3, 7],\n",
    "    'rf__max_features': [10, 50],\n",
    "    'rf__min_samples_split': [1000, 2000],\n",
    "    'rf__min_samples_leaf':[30, 60],\n",
    "    'rf__n_jobs': [-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada Boost Model Tuning Hyperparameters\n",
    "# source: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "ada_params = {\n",
    "    'ada__n_estimators': [40, 100],\n",
    "    'ada__learning_rate': [0.1, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run model -- input vectorizer and model\n",
    "def run_model(vec, mod, vec_params={}, mod_params={}, grid_search=False):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "            (vec, vectorizers[vec]),\n",
    "            (mod, models[mod])\n",
    "            ])\n",
    "    \n",
    "    if grid_search:\n",
    "        gs = GridSearchCV(pipe, param_grid = {**vec_params, **mod_params}, cv=3, verbose=1, n_jobs=-1)\n",
    "        gs.fit(X_train, y_train)\n",
    "        pipe = gs.best_estimator_\n",
    "    else:\n",
    "        pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Retrieve metrics\n",
    "    results['model'] = mod\n",
    "    results['vectorizer'] = vec\n",
    "    results['train'] = pipe.score(X_train, y_train)\n",
    "    results['test'] = pipe.score(X_test, y_test)\n",
    "    predictions = pipe.predict(X_test)\n",
    "    prob = pipe.predict_proba(X_test)[:,1]\n",
    "    results['roc'] = roc_auc_score(y_test, prob)\n",
    "    results['precision'] = precision_score(y_test, predictions)\n",
    "    results['recall'] = recall_score(y_test, predictions)\n",
    "    results['f_score'] = f1_score(y_test, predictions)\n",
    "    \n",
    "    if grid_search:\n",
    "        tuning_list.append(results)\n",
    "        print('### BEST PARAMS ###')\n",
    "        display(pipe)\n",
    "    else:\n",
    "        eval_list.append(results)\n",
    "    \n",
    "    print('### METRICS ###')\n",
    "    display(results)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Benchmark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark for Combined Title on Daily Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.583665\n",
       "0    0.416335\n",
       "Name: percent_change_class, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailytitle_df['percent_change_class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark for Combined titlepost & title for 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.592558\n",
       "0    0.407442\n",
       "Name: percent_change_class, dtype: float64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlepost_df['percent_change_class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Title on Daily Basis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailytitle_df.shape[0] * (3/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dailytitle_df['title_cleaned']\n",
    "y = dailytitle_df['percent_change_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.iloc[0:377]\n",
    "X_test = X.iloc[377:]\n",
    "y_train = y.iloc[0:377]\n",
    "y_test = y.iloc[377:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of X_train: (377,)\n",
      "The Shape of X_test: (125,)\n",
      "The Shape of y_train: (377,)\n",
      "The Shape of y_test: (125,)\n"
     ]
    }
   ],
   "source": [
    "print(f'The Shape of X_train: {X_train.shape}')\n",
    "print(f'The Shape of X_test: {X_test.shape}')\n",
    "print(f'The Shape of y_train: {y_train.shape}')\n",
    "print(f'The Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_list = []\n",
    "tuning_list =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Modelling the following functions were used:\n",
    "\n",
    "Vectorizer:\n",
    "- cvec\n",
    "- tvec\n",
    "\n",
    "Model:\n",
    "- Logistic Regression\n",
    "- Naive Bayes multinomial\n",
    "- Random Forest\n",
    "- Ada Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression w cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.95, max_features=100, min_df=4,\n",
       "                                 ngram_range=(2, 2), stop_words='english')),\n",
       "                ('lr', LogisticRegression(C=1, random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'lr',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.7798408488063661,\n",
       " 'test': 0.488,\n",
       " 'roc': 0.42228661749209695,\n",
       " 'precision': 0.56,\n",
       " 'recall': 0.5753424657534246,\n",
       " 'f_score': 0.5675675675675674}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 19\n",
      "False Positives: 33\n",
      "False Negatives: 31\n",
      "True Positives: 42\n"
     ]
    }
   ],
   "source": [
    "cvec_lr_gs = run_model('cvec', 'lr', vec_params=cvec_params, mod_params=lr_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression w tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=0.4, max_features=100, min_df=3,\n",
       "                                 ngram_range=(2, 3), stop_words='english')),\n",
       "                ('lr', LogisticRegression(C=1, random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'lr',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.7108753315649867,\n",
       " 'test': 0.568,\n",
       " 'roc': 0.5076396206533194,\n",
       " 'precision': 0.584070796460177,\n",
       " 'recall': 0.9041095890410958,\n",
       " 'f_score': 0.7096774193548386}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 5\n",
      "False Positives: 47\n",
      "False Negatives: 7\n",
      "True Positives: 66\n"
     ]
    }
   ],
   "source": [
    "tvec_lr_gs = run_model('tvec', 'lr', vec_params=tvec_params, mod_params=lr_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB w cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.95, max_features=100, min_df=2,\n",
       "                                 ngram_range=(2, 3), stop_words='english')),\n",
       "                ('nb', MultinomialNB(alpha=0.4, fit_prior=False))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'nb',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.6657824933687002,\n",
       " 'test': 0.504,\n",
       " 'roc': 0.44863013698630133,\n",
       " 'precision': 0.5632183908045977,\n",
       " 'recall': 0.6712328767123288,\n",
       " 'f_score': 0.6125}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 14\n",
      "False Positives: 38\n",
      "False Negatives: 24\n",
      "True Positives: 49\n"
     ]
    }
   ],
   "source": [
    "cvec_nb_gs = run_model('cvec', 'nb', vec_params=cvec_params, mod_params=nb_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB w tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=0.4, max_features=100, min_df=3,\n",
       "                                 ngram_range=(2, 3), stop_words='english')),\n",
       "                ('nb', MultinomialNB(alpha=0.4))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'nb',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.6339522546419099,\n",
       " 'test': 0.6,\n",
       " 'roc': 0.5168598524762908,\n",
       " 'precision': 0.5934959349593496,\n",
       " 'recall': 1.0,\n",
       " 'f_score': 0.7448979591836735}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 2\n",
      "False Positives: 50\n",
      "False Negatives: 0\n",
      "True Positives: 73\n"
     ]
    }
   ],
   "source": [
    "tvec_nb_gs = run_model('tvec', 'nb', vec_params=tvec_params, mod_params=nb_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest w cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1280 candidates, totalling 3840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed: 25.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed: 32.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3840 out of 3840 | elapsed: 41.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.2, min_df=2, stop_words='english')),\n",
       "                ('rf',\n",
       "                 RandomForestClassifier(max_depth=3, max_features=10,\n",
       "                                        min_samples_leaf=30,\n",
       "                                        min_samples_split=1000, n_jobs=-1,\n",
       "                                        random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'rf',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.583554376657825,\n",
       " 'test': 0.584,\n",
       " 'roc': 0.5,\n",
       " 'precision': 0.584,\n",
       " 'recall': 1.0,\n",
       " 'f_score': 0.7373737373737373}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 0\n",
      "False Positives: 52\n",
      "False Negatives: 0\n",
      "True Positives: 73\n"
     ]
    }
   ],
   "source": [
    "cvec_rf_gs = run_model('cvec', 'rf', vec_params=cvec_params, mod_params=rf_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest w tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1280 candidates, totalling 3840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed: 25.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed: 34.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3840 out of 3840 | elapsed: 41.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=0.2, min_df=3, stop_words='english')),\n",
       "                ('rf',\n",
       "                 RandomForestClassifier(max_depth=3, max_features=10,\n",
       "                                        min_samples_leaf=30,\n",
       "                                        min_samples_split=1000, n_jobs=-1,\n",
       "                                        random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'rf',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.583554376657825,\n",
       " 'test': 0.584,\n",
       " 'roc': 0.5,\n",
       " 'precision': 0.584,\n",
       " 'recall': 1.0,\n",
       " 'f_score': 0.7373737373737373}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 0\n",
      "False Positives: 52\n",
      "False Negatives: 0\n",
      "True Positives: 73\n"
     ]
    }
   ],
   "source": [
    "tvec_rf_gs = run_model('tvec', 'rf', vec_params=tvec_params, mod_params=rf_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADA Boost w cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.95, min_df=4, ngram_range=(2, 3),\n",
       "                                 stop_words='english')),\n",
       "                ('ada',\n",
       "                 AdaBoostClassifier(learning_rate=0.1, n_estimators=40,\n",
       "                                    random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'ada',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.7771883289124668,\n",
       " 'test': 0.56,\n",
       " 'roc': 0.5317439409905164,\n",
       " 'precision': 0.5882352941176471,\n",
       " 'recall': 0.821917808219178,\n",
       " 'f_score': 0.6857142857142857}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 10\n",
      "False Positives: 42\n",
      "False Negatives: 13\n",
      "True Positives: 60\n"
     ]
    }
   ],
   "source": [
    "cvec_ada_gs = run_model('cvec', 'ada', vec_params=cvec_params, mod_params=ada_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADA Boost w tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=0.4, max_features=100, min_df=3,\n",
       "                                 ngram_range=(2, 3), stop_words='english')),\n",
       "                ('ada',\n",
       "                 AdaBoostClassifier(learning_rate=1, n_estimators=40,\n",
       "                                    random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'ada',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.8541114058355438,\n",
       " 'test': 0.528,\n",
       " 'roc': 0.5021074815595363,\n",
       " 'precision': 0.574468085106383,\n",
       " 'recall': 0.7397260273972602,\n",
       " 'f_score': 0.6467065868263473}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 12\n",
      "False Positives: 40\n",
      "False Negatives: 19\n",
      "True Positives: 54\n"
     ]
    }
   ],
   "source": [
    "tvec_ada_gs = run_model('tvec', 'ada', vec_params=tvec_params, mod_params=ada_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most model, with the feature as combined title on the daily basis, have an average improvement against the benchmark at approximately 61%. Running through both vectorizers, the count vectorizer tend to be overfitted while the tdif vectorizer is performing less overfitting as compared to count vectorizer.\n",
    "\n",
    "Among all four models, the Multinominal NB with Tdif vectorizer performs the best in terms of accuracy and it is the least overfitted looking the train-test result. While the ROC-AUC score may not be the top, the remaining factors like the Precision score, Recall score and the f1 score is among the best against the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_eval_df = pd.DataFrame(tuning_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Evaluated List into a DataFrame\n",
    "title_eval_df.to_csv('data/dailytitle_eval_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>roc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.779841</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.422287</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.575342</td>\n",
       "      <td>0.567568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lr</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.710875</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.507640</td>\n",
       "      <td>0.584071</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>0.709677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.665782</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.448630</td>\n",
       "      <td>0.563218</td>\n",
       "      <td>0.671233</td>\n",
       "      <td>0.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nb</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.633952</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.516860</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.583554</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.737374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.583554</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.737374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ada</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.777188</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.531744</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ada</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.854111</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.502107</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.646707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model vectorizer     train   test       roc  precision    recall   f_score\n",
       "0    lr       cvec  0.779841  0.488  0.422287   0.560000  0.575342  0.567568\n",
       "1    lr       tvec  0.710875  0.568  0.507640   0.584071  0.904110  0.709677\n",
       "2    nb       cvec  0.665782  0.504  0.448630   0.563218  0.671233  0.612500\n",
       "3    nb       tvec  0.633952  0.600  0.516860   0.593496  1.000000  0.744898\n",
       "4    rf       cvec  0.583554  0.584  0.500000   0.584000  1.000000  0.737374\n",
       "5    rf       tvec  0.583554  0.584  0.500000   0.584000  1.000000  0.737374\n",
       "6   ada       cvec  0.777188  0.560  0.531744   0.588235  0.821918  0.685714\n",
       "7   ada       tvec  0.854111  0.528  0.502107   0.574468  0.739726  0.646707"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_model_result = pd.read_csv('data/dailytitle_eval_df.csv')\n",
    "title_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Title & Post for 2019\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67482\n",
      "50611.5\n"
     ]
    }
   ],
   "source": [
    "print(titlepost_df.shape[0])\n",
    "print(titlepost_df.shape[0] *(3/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titlepost_df['lean_meat']\n",
    "y = titlepost_df['percent_change_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.iloc[0:50611]\n",
    "X_test = X.iloc[50611:]\n",
    "y_train = y.iloc[0:50611]\n",
    "y_test = y.iloc[50611:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of X_train: (50611,)\n",
      "The Shape of X_test: (16871,)\n",
      "The Shape of y_train: (50611,)\n",
      "The Shape of y_test: (16871,)\n"
     ]
    }
   ],
   "source": [
    "print(f'The Shape of X_train: {X_train.shape}')\n",
    "print(f'The Shape of X_test: {X_test.shape}')\n",
    "print(f'The Shape of y_train: {y_train.shape}')\n",
    "print(f'The Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_list = []\n",
    "tuning_list =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Modelling the following functions were used:\n",
    "\n",
    "Vectorizer:\n",
    "- cvec\n",
    "- tvec\n",
    "\n",
    "Model:\n",
    "- Logistic Regression\n",
    "- Naive Bayes multinomial\n",
    "- Random Forest\n",
    "- Ada Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression w cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.2, max_features=100, min_df=4,\n",
       "                                 ngram_range=(3, 3), stop_words='english')),\n",
       "                ('lr', LogisticRegression(C=0.1, random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'lr',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.5870067771828259,\n",
       " 'test': 0.6101001718925968,\n",
       " 'roc': 0.5003924979847831,\n",
       " 'precision': 0.6101001718925968,\n",
       " 'recall': 1.0,\n",
       " 'f_score': 0.7578412604918275}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 0\n",
      "False Positives: 6578\n",
      "False Negatives: 0\n",
      "True Positives: 10293\n"
     ]
    }
   ],
   "source": [
    "cvec_lr_gs = run_model('cvec', 'lr', vec_params=cvec_params, mod_params=lr_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression w tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=0.2, min_df=3, ngram_range=(3, 3),\n",
       "                                 stop_words='english')),\n",
       "                ('lr', LogisticRegression(C=0.1, random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'lr',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.5868684673292367,\n",
       " 'test': 0.610040898583368,\n",
       " 'roc': 0.5065857764874403,\n",
       " 'precision': 0.610077059869591,\n",
       " 'recall': 0.9999028465947731,\n",
       " 'f_score': 0.7577955306851233}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 0\n",
      "False Positives: 6578\n",
      "False Negatives: 1\n",
      "True Positives: 10292\n"
     ]
    }
   ],
   "source": [
    "tvec_lr_gs = run_model('tvec', 'lr', vec_params=tvec_params, mod_params=lr_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB w cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.2, max_features=100, min_df=2,\n",
       "                                 ngram_range=(3, 3), stop_words='english')),\n",
       "                ('nb', MultinomialNB(alpha=0.8))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'nb',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.5861571595107783,\n",
       " 'test': 0.6101594452018256,\n",
       " 'roc': 0.5003925570625607,\n",
       " 'precision': 0.610149395304719,\n",
       " 'recall': 0.9999028465947731,\n",
       " 'f_score': 0.7578513309524686}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 2\n",
      "False Positives: 6576\n",
      "False Negatives: 1\n",
      "True Positives: 10292\n"
     ]
    }
   ],
   "source": [
    "cvec_nb_gs = run_model('cvec', 'nb', vec_params=cvec_params, mod_params=nb_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB w tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=0.2, max_features=100, min_df=5,\n",
       "                                 ngram_range=(3, 3), stop_words='english')),\n",
       "                ('nb', MultinomialNB(alpha=0.8))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'nb',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.5867301574756476,\n",
       " 'test': 0.6101001718925968,\n",
       " 'roc': 0.5003926604486715,\n",
       " 'precision': 0.6101001718925968,\n",
       " 'recall': 1.0,\n",
       " 'f_score': 0.7578412604918275}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 0\n",
      "False Positives: 6578\n",
      "False Negatives: 0\n",
      "True Positives: 10293\n"
     ]
    }
   ],
   "source": [
    "tvec_nb_gs = run_model('tvec', 'nb', vec_params=tvec_params, mod_params=nb_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest w cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1280 candidates, totalling 3840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed: 33.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed: 44.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed: 52.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3840 out of 3840 | elapsed: 61.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.2, min_df=2, stop_words='english')),\n",
       "                ('rf',\n",
       "                 RandomForestClassifier(max_depth=3, max_features=10,\n",
       "                                        min_samples_leaf=30,\n",
       "                                        min_samples_split=1000, n_jobs=-1,\n",
       "                                        random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'rf',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.5867103989251349,\n",
       " 'test': 0.6101001718925968,\n",
       " 'roc': 0.5030790156118048,\n",
       " 'precision': 0.6101001718925968,\n",
       " 'recall': 1.0,\n",
       " 'f_score': 0.7578412604918275}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 0\n",
      "False Positives: 6578\n",
      "False Negatives: 0\n",
      "True Positives: 10293\n"
     ]
    }
   ],
   "source": [
    "cvec_rf_gs = run_model('cvec', 'rf', vec_params=cvec_params, mod_params=rf_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest w tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1280 candidates, totalling 3840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed: 28.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed: 44.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3840 out of 3840 | elapsed: 54.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=0.2, max_features=100, min_df=5,\n",
       "                                 ngram_range=(1, 2), stop_words='english')),\n",
       "                ('rf',\n",
       "                 RandomForestClassifier(max_depth=7, max_features=50,\n",
       "                                        min_samples_leaf=60,\n",
       "                                        min_samples_split=1000, n_jobs=-1,\n",
       "                                        random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'rf',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.5886664954258956,\n",
       " 'test': 0.6093888921818505,\n",
       " 'roc': 0.5074267264970952,\n",
       " 'precision': 0.610477952145116,\n",
       " 'recall': 0.9939764888759351,\n",
       " 'f_score': 0.7563950909359751}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 50\n",
      "False Positives: 6528\n",
      "False Negatives: 62\n",
      "True Positives: 10231\n"
     ]
    }
   ],
   "source": [
    "tvec_rf_gs = run_model('tvec', 'rf', vec_params=tvec_params, mod_params=rf_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADA Boost w cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.2, min_df=2, stop_words='english')),\n",
       "                ('ada',\n",
       "                 AdaBoostClassifier(learning_rate=0.1, n_estimators=40,\n",
       "                                    random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'ada',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.5870067771828259,\n",
       " 'test': 0.610040898583368,\n",
       " 'roc': 0.5015471879169875,\n",
       " 'precision': 0.610077059869591,\n",
       " 'recall': 0.9999028465947731,\n",
       " 'f_score': 0.7577955306851233}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 0\n",
      "False Positives: 6578\n",
      "False Negatives: 1\n",
      "True Positives: 10292\n"
     ]
    }
   ],
   "source": [
    "cvec_ada_gs = run_model('cvec', 'ada', vec_params=cvec_params, mod_params=ada_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADA Boost w tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BEST PARAMS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=0.2, max_features=100, min_df=3,\n",
       "                                 stop_words='english')),\n",
       "                ('ada',\n",
       "                 AdaBoostClassifier(learning_rate=0.1, n_estimators=40,\n",
       "                                    random_state=42))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### METRICS ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'ada',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.5881330145620517,\n",
       " 'test': 0.6106929049848853,\n",
       " 'roc': 0.5029328424206327,\n",
       " 'precision': 0.6106194690265486,\n",
       " 'recall': 0.9988341591372778,\n",
       " 'f_score': 0.7579063767047548}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 22\n",
      "False Positives: 6556\n",
      "False Negatives: 12\n",
      "True Positives: 10281\n"
     ]
    }
   ],
   "source": [
    "tvec_ada_gs = run_model('tvec', 'ada', vec_params=tvec_params, mod_params=ada_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlepost_eval_df = pd.DataFrame(tuning_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Evaluated List into a DataFrame\n",
    "titlepost_eval_df.to_csv('data/titlepost_eval_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>roc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.779841</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.422287</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.575342</td>\n",
       "      <td>0.567568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lr</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.710875</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.507640</td>\n",
       "      <td>0.584071</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>0.709677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.665782</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.448630</td>\n",
       "      <td>0.563218</td>\n",
       "      <td>0.671233</td>\n",
       "      <td>0.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nb</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.633952</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.516860</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.583554</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.737374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.583554</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.737374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ada</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.777188</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.531744</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ada</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.854111</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.502107</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.646707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model vectorizer     train   test       roc  precision    recall   f_score\n",
       "0    lr       cvec  0.779841  0.488  0.422287   0.560000  0.575342  0.567568\n",
       "1    lr       tvec  0.710875  0.568  0.507640   0.584071  0.904110  0.709677\n",
       "2    nb       cvec  0.665782  0.504  0.448630   0.563218  0.671233  0.612500\n",
       "3    nb       tvec  0.633952  0.600  0.516860   0.593496  1.000000  0.744898\n",
       "4    rf       cvec  0.583554  0.584  0.500000   0.584000  1.000000  0.737374\n",
       "5    rf       tvec  0.583554  0.584  0.500000   0.584000  1.000000  0.737374\n",
       "6   ada       cvec  0.777188  0.560  0.531744   0.588235  0.821918  0.685714\n",
       "7   ada       tvec  0.854111  0.528  0.502107   0.574468  0.739726  0.646707"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlepost_model_result = pd.read_csv('data/dailytitle_eval_df.csv')\n",
    "titlepost_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Insights\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After thorough data cleaning and exploratory analysis, we ran 2 different vectorizers (count vectorizer & tdif vectorizer) along with 4 different models using Logistic Regression, Multinomial NB, Random Forest and AdaBoost. \n",
    "\n",
    "For model evaluation, we recorded train and test scores, ROC-AUC scores, specificity, sensitivity and F1 scores for each of the models. Based on the accuracy score, the Multinomial NB model with tvec has the best accuracy. However, it is only doing slightly better than the benchmark due to the high similarity between the two classes of up and down trend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown during the EDA process, there are too many common words between the up and down class regardless if it is within the uni-gram, bi-gram or tri-gram. Additionally using NLTK Vader, the returns of the results indicate that despite being in either the positive or negative sentiment, there will still be a mixture of up and down trends to either of the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few recommendation could potentially improve the accuracy of the model:\n",
    "- Using other input from the post (number of comments, number of votes to the post (positive/negative)\n",
    "- Increase the dataset to include post from official news outlets reporting on stock market\n",
    "- Increase the dataset by including post from influential people as proven that a single post from influential figure will cost a certain companies stock to surge. (Source: https://www.straitstimes.com/business/companies-markets/gamestop-market-value-soars-past-13-billion-after-elon-musk-tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most models were performing slightly better than the benchmark, the accuracy can be further improved. The sentiment is a balanced mix in the realm of positivity and negativity regardless of a bearish or bullish trend in the S&P500, the reddit community cannot determine whether S&P 500 will be going up or down. There will always be people buying and selling whenever it is a trading day.\n",
    "\n",
    "Comparing 2019 and 2020, the most prominent explanation would be due to Covid. As Covid surged the number of posts when comparing year-on-year and there was a huge market correction in Mar 2020. This cost a mixed reaction by the investors as two groups of investor's appear which results in buying and selling regardless of the performance of the S&P 500. This is possible due to some seeing it as an opportunity to buy stocks at a lower price while some fear a potential market crash hence panic sell occured. \n",
    "\n",
    "Overall, the EDA process is a very good indicator that there will be a huge challenge in producing a good model due to the huge similarity in the features against two different classifiers. There are also additional insights to consider to be part of the feature like comparing the number of posts during a certain period of time to better classify between up and down trends.\n",
    "\n",
    "While the limitation is due to the huge similarity between both post, this could potentially be overcome by increasing the no of features to aid classifying between up or down trend of the stock market. Additionally, official or influential figures can also affect how the stock market performs for the day. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
